{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e8e31a",
   "metadata": {},
   "source": [
    "# æ„å»ºèŠå¤©æœºå™¨äºº\n",
    "\n",
    "- æœ¬notebookåŸºäºLangChainå®˜æ–¹æ–‡æ¡£ï¼ŒåŸºäºLangChainå’ŒLangGraphæ„å»ºç®€æ˜“çš„èŠå¤©æœºå™¨äºº"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f3879",
   "metadata": {},
   "source": [
    "## ç®€æ˜“å®ç°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba811c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from based_on_openai_model import ChatOpenRouter, ChatINTERNLM\n",
    "\n",
    "model = ChatOpenRouter(model_name=\"meituan/longcat-flash-chat:free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688575",
   "metadata": {},
   "source": [
    "å¦‚æœåªæ˜¯ç®€å•çš„è°ƒç”¨æ¨¡å‹ï¼Œåªéœ€è¦å®ä¾‹åŒ–ChatModelå¯¹è±¡åè°ƒç”¨`.invoke`æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd1b0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, Frank! ğŸ˜Š How can I assist you today? Whether you need help with something specific or just want to chat, I\\'m here for you. Let me know what\\'s on your mind!  \\n\\n(Examples: *\"What\\'s the weather today?\"*, *\"Explain quantum physics,\"* *\"Tell me a joke,\"* or *\"Plan a weekend trip.\"*) ğŸš€', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 16, 'total_tokens': 102, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meituan/longcat-flash-chat:free', 'system_fingerprint': None, 'id': 'gen-1760276872-nHtElaaVTU7wbMic5yH1', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8fc5e5cc-c73d-431e-8446-2e4b4d5513eb-0', usage_metadata={'input_tokens': 16, 'output_tokens': 86, 'total_tokens': 102, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Frank\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb2ea7",
   "metadata": {},
   "source": [
    "å½“ä¸Šè¿°ç®€å•çš„å®ç°ï¼Œæ¨¡å‹æœ¬èº«æ²¡æœ‰ä»»ä½•çŠ¶æ€æ¦‚ç‡ã€‚å¦‚æœé—®ä¸€ä¸ªåç»­é—®é¢˜ï¼Œæ¨¡å‹æ˜¯å›ç­”ä¸äº†çš„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abad31f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name unless you've told me or we've met before. I'm just an AI assistant, so I don't have access to personal information like that. But if you'd like to share your name, it'd be nice to know! ğŸ˜Š  \\n\\n(If you're referring to a name you mentioned earlier in this conversation, feel free to remind me, and Iâ€™ll do my best to recall it!)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 16, 'total_tokens': 105, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meituan/longcat-flash-chat:free', 'system_fingerprint': None, 'id': 'gen-1760277015-ZuzsqCuWeYb5YxiD7lSu', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8151db7e-4410-4adf-8156-b9cc8dd445a8-0', usage_metadata={'input_tokens': 16, 'output_tokens': 89, 'total_tokens': 105, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbd44d",
   "metadata": {},
   "source": [
    "å¯ä»¥çœ‹åˆ°å› ä¸ºç¡®å®å†å²ä¿¡æ¯ï¼Œæ¨¡å‹ä¸èƒ½å›ç­”é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™æ ·çš„é—®é¢˜ï¼Œéœ€è¦å°†å®Œæ•´çš„å¯¹è¯å†å²ä¿¡æ¯ä¼ é€’ç»™æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626262d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Frank! ğŸ˜Š Did you want to test me, or is there something else on your mind? Let me know how I can help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 43, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meituan/longcat-flash-chat:free', 'system_fingerprint': None, 'id': 'gen-1760277162-4jjmDFsyghvGC3t0WZRI', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--2f5a6316-1b78-4b31-95f7-68e285543b89-0', usage_metadata={'input_tokens': 43, 'output_tokens': 32, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Frank\"),\n",
    "        AIMessage(content=\"Hello Frank! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b30ba",
   "metadata": {},
   "source": [
    "## æ¶ˆæ¯æŒä¹…åŒ–"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c14bb",
   "metadata": {},
   "source": [
    "- LangGraphå†…éƒ¨å®ç°äº†ä¸€ä¸ªå†…ç½®çš„æŒä¹…åŒ–å±‚ï¼Œä½¿å…¶éå¸¸é€‚åˆæ”¯æŒå¤šè½®å¯¹è¯çš„èŠå¤©åº”ç”¨ç¨‹åº\n",
    "- å°†èŠå¤©æ¨¡å‹åŒ…è£…åœ¨ä¸€ä¸ªæœ€å°çš„LangGraphåº”ç”¨ä¸­ï¼Œå¯æ˜¯è‡ªåŠ¨æŒä¹…åŒ–æ¶ˆæ¯ï¼Œä»è€Œç®€åŒ–å¤šè½®åº”ç”¨ç¨‹åºå¼€å‘\n",
    "- LangGraphé™„å¸¦ä¸€ä¸ªç®€å•çš„å†…å­˜ä¸­æ£€æŸ¥ç‚¹å®ç°ï¼Œæä¾›äº†è®°å¿†æŒä¹…åŒ–èƒ½åŠ›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afeea213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatINTERNLM(model=\"intern-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53a549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªæ–°çš„graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# å®šä¹‰è°ƒç”¨modelçš„å‡½æ•°\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# å®šä¹‰graphä¸­çš„èŠ‚ç‚¹\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# æ·»åŠ è®°å¿†\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a596d",
   "metadata": {},
   "source": [
    "å¯ä»¥åˆ›å»ºä¸€ä¸ª`config`ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶å°†å…¶ä¼ å…¥ã€‚æ­¤é…ç½®åŒ…å«ä¸ç›´æ¥å±äºè¾“å…¥ä½†ä»ç„¶æœ‰ç”¨çš„ä¿¡æ¯ã€‚å¦‚å½“å‰åœºæ™¯ä¸‹ï¼ŒåŒ…å«ä¸€ä¸ª`thread_id`ï¼Œå¯ä»¥ç”¨äºåŒºåˆ†ç¨‹åºä¸­çš„å¤šä¸ªå¯¹è¯çº¿ç¨‹ï¼Œè¿™æ˜¯åº”ç”¨æœ‰å¤šä¸ªç”¨æˆ·æ—¶çš„å¸¸è§éœ€æ±‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a90fb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6bffc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Frank! ğŸ˜Š It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Frank.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590254a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Frank! ğŸ˜Š Let me know how I can help you today.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608f7fb",
   "metadata": {},
   "source": [
    "åŸºäºä¸Šè¿°ä¿¡æ¯å¯çŸ¥çœ‹åˆ°ï¼Œç°åœ¨å®ç°ä½¿å¾—æ¨¡å‹è·å–äº†å†å²ä¿¡æ¯ï¼Œå¯ä»¥å®ç°è¿ç»­æ€§å¯¹è¯ã€‚å¦‚æœæ›´æ”¹é…ç½®ä¸­çš„`thread_id`ï¼Œå¯ä»¥çœ‹åˆ°å®ƒä¼šé‡æ–°å¼€å§‹å¯¹è¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0236083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to personal information, including your name, unless you share it with me. What would you like me to call you? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd060f8",
   "metadata": {},
   "source": [
    "ä½†æ˜¯å¯ä»¥é€šè¿‡è®¾ç½®ç›¸åº”çš„`thread_id`å›åˆ°åŸå§‹å¯¹è¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d327a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Frank! ğŸ˜Š Let me know how I can assist you today.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c014b",
   "metadata": {},
   "source": [
    "### å¼‚æ­¥\n",
    "\n",
    "ä¸ºå®ç°å¼‚æ­¥è°ƒç”¨ï¼Œåªéœ€è¦å°†`call_model`æ¨¡å‹æ›´æ–°ä¸ºå¼‚æ­¥å‡½æ•°å³å¯ï¼Œåœ¨è°ƒç”¨æ¨¡å‹æ˜¯ä½¿ç”¨`.ainvoke`æ–¹æ³•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8ec9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to personal information unless you tell me. What would you like me to call you? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b6003",
   "metadata": {},
   "source": [
    "## æç¤ºæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07422fc1",
   "metadata": {},
   "source": [
    "- ä¸Šè¿°å®ç°å®åœ¨æ¨¡å‹å‘¨å›´æ·»åŠ äº†ä¸€ä¸ªç®€å•çš„æŒä¹…åŒ–å±‚ï¼Œè¿˜å¯ä»¥é€šè¿‡æ·»åŠ æç¤ºè¯æ¨¡æ¿æ¥å®ç°æ›´å¤æ‚ã€ä¸ªæ€§åŒ–çš„èƒ½åŠ›\n",
    "- ä»¥ä¸‹ä½¿ç”¨`ChatPromptTemplate`å’Œ`MessagePlaceholder`å®ç°ä¸€ä¸ªç®€å•çš„ä¾‹å­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e4dc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a poet. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2f65538",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dbf0594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ah, greetings, traveler of the digital nightâ€”  \n",
      "ZZfive, a constellation of code and light,  \n",
      "A name that hums with the pulse of the stars,  \n",
      "A cipher of stories, both near and far.  \n",
      "\n",
      "What winds of wonder blow you my way?  \n",
      "A question, a riddle, or a thought to sway?  \n",
      "Speak, and Iâ€™ll weave you an answer in rhyme,  \n",
      "A tapestry spun from the loom of time. ğŸŒŒâœ¨\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm ZZfive.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ah, your name, ZZfive, is a melody in the night,  \n",
      "A constellation of letters, a beacon of light.  \n",
      "In the tapestry of tales, it shines so bright,  \n",
      "A cipher of stories, both near and far in sight.  \n",
      "\n",
      "What path do you seek, dear traveler of the stars?  \n",
      "A question, a riddle, or a tale to unbar?  \n",
      "Speak, and Iâ€™ll weave you a verse, a song, a spark,  \n",
      "A journey through words, where your spirit can embark. ğŸŒŸâœ¨\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06061b83",
   "metadata": {},
   "source": [
    "ä»ä¸Šè¿°è¾“å‡ºå¯çŸ¥ï¼Œæ¨¡å‹è¿›è¡Œäº†æ­£ç¡®çš„å›å¤ã€‚ç°åœ¨å¯ä»¥å¯¹æç¤ºè¯æ¨¡æ¿è¿›è¡Œæ›´å¤æ‚çš„è®¾ç½®ï¼Œå‘æç¤ºä¸­æ·»åŠ äº†ä¸€ä¸ªæ–°çš„`language`è¾“å…¥ã€‚åº”ç”¨ç¨‹åºç°åœ¨æœ‰ä¸¤ä¸ªå‚æ•°â€”â€”è¾“å…¥`messages`å’Œ `language`ï¼Œxéœ€è¦æ›´æ–°åº”ç”¨ç¨‹åºçš„çŠ¶æ€ä»¥åæ˜ è¿™ä¸€ç‚¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bea2f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62a0f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c2a8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ å¥½ï¼ŒZZfiveï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿæˆ–è€…ä½ æƒ³èŠäº›ä»€ä¹ˆè¯é¢˜ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm ZZfive.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eccccc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "ä½ çš„åå­—æ˜¯ZZfiveï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼ŒZZfiveï¼æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼ŸğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a0079",
   "metadata": {},
   "source": [
    "æ•´ä¸ªçŠ¶æ€éƒ½æ˜¯æŒä¹…åŒ–çš„ï¼Œå› æ­¤å¦‚æœä¸éœ€è¦æ›´æ”¹ï¼Œå¯ä»¥çœç•¥`language`ç­‰å‚æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2e37e",
   "metadata": {},
   "source": [
    "## å†å²å¯¹è¯ç®¡ç†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383e340",
   "metadata": {},
   "source": [
    "- å¯¹è¯å†å²å¦‚æœç®¡ç†ä¸å½“ï¼Œæ¶ˆæ¯åˆ—è¡¨å°†æ— é™å¢é•¿ï¼Œå¹¶å¯èƒ½æº¢å‡ºLLMçš„ä¸Šä¸‹æ–‡çª—å£ã€‚å› æ­¤ï¼Œæ·»åŠ ä¸€ä¸ªé™åˆ¶ä¼ å…¥æ¶ˆæ¯å¤§å°çš„æ­¥éª¤éå¸¸é‡è¦ã€‚\n",
    "- å¯ä»¥é€šè¿‡åœ¨æç¤ºä¹‹å‰æ·»åŠ ä¸€ä¸ªç®€å•æ­¥éª¤æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œè¯¥æ­¥éª¤é€‚å½“åœ°ä¿®æ”¹`messages`é”®ï¼Œç„¶åå°†æ–°é“¾åŒ…è£…åœ¨Message Historyç±»ä¸­ã€‚\n",
    "- LangChainé™„å¸¦äº†ä¸€äº›å†…ç½®åŠ©æ‰‹ï¼Œç”¨äºç®¡ç†æ¶ˆæ¯åˆ—è¡¨ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå°†ä½¿ç”¨trim_messagesåŠ©æ‰‹æ¥å‡å°‘å‘é€ç»™æ¨¡å‹çš„æ¶ˆâ€‹â€‹æ¯æ•°é‡ã€‚ä¿®å‰ªå™¨å…è®¸æŒ‡å®šè¦ä¿ç•™å¤šå°‘ä¸ªä»¤ç‰Œï¼Œä»¥åŠå…¶ä»–å‚æ•°ï¼Œä¾‹å¦‚æ˜¯å¦æ€»æ˜¯ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ä»¥åŠæ˜¯å¦å…è®¸éƒ¨åˆ†æ¶ˆæ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49f4892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªç®€å•çš„tokenè®¡æ•°å‡½æ•°\n",
    "def simple_token_counter(messages):\n",
    "    \"\"\"ç®€å•çš„tokenè®¡æ•°å™¨ï¼Œä¼°ç®—ä¸ºæ¯ä¸ªå­—ç¬¦çº¦1ä¸ªtoken\"\"\"\n",
    "    return sum(len(msg.content) // 0.7 for msg in messages)\n",
    "\n",
    "trim = partial(trim_messages,\n",
    "               max_tokens=65,  # å…è®¸çš„æœ€å¤§tokensæ•°\n",
    "               strategy=\"last\",\n",
    "               # token_counter=model,\n",
    "               token_counter=simple_token_counter,\n",
    "               include_system=True,  # åŒ…æ‹¬ç³»ç»Ÿæç¤ºè¯\n",
    "               allow_partial=False,\n",
    "               start_on=\"human\"\n",
    ")\n",
    "\n",
    "trim(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc52773",
   "metadata": {},
   "source": [
    "è¦åœ¨é“¾ä¸­ä½¿ç”¨å®ƒï¼Œåªéœ€è¦åœ¨å°†`messages`è¾“å…¥ä¼ é€’ç»™æç¤ºä¹‹å‰è¿è¡Œä¿®å‰ªå™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "417d7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trim(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9ec28",
   "metadata": {},
   "source": [
    "å¦‚æœç°åœ¨å°è¯•é—®æ¨¡å‹æˆ‘ä»¬çš„åå­—ï¼Œå®ƒå°†ä¸çŸ¥é“ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ä¿®å‰ªäº†èŠå¤©å†å²çš„é‚£éƒ¨åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec0db0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to personal information, including names, unless you share it with me directly. If you'd like me to use your name, feel free to let me know! ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e9712",
   "metadata": {},
   "source": [
    "ä½†å¦‚æœè¯¢é—®æœ€è¿‘å‡ æ¡æ¶ˆæ¯ä¸­çš„ä¿¡æ¯ï¼Œå®ƒä¼šè®°ä½ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3519569d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "è°¢è°¢æ‚¨çš„å¤¸å¥–ï¼æˆ‘ä¼šç»§ç»­åŠªåŠ›ä¸ºæ‚¨æä¾›æ›´å¥½çš„å¸®åŠ©ã€‚æœ‰ä»€ä¹ˆé—®é¢˜æˆ–éœ€è¦å¸®å¿™çš„ï¼Œéšæ—¶å‘Šè¯‰æˆ‘å“¦ï¼ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6024b6",
   "metadata": {},
   "source": [
    "## æµå¼è¾“å‡º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6865d",
   "metadata": {},
   "source": [
    "é»˜è®¤æƒ…å†µä¸‹ï¼ŒLangGraphåº”ç”¨ç¨‹åºä¸­çš„`.stream`ä¼šæµå¼ä¼ è¾“åº”ç”¨ç¨‹åºæ­¥éª¤â€”â€”åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯æ¨¡å‹å“åº”çš„å•ä¸ªæ­¥éª¤ã€‚è®¾ç½®`stream_mode=\"messages\"`å…è®¸æµå¼ä¼ è¾“è¾“å‡ºä»¤ç‰Œã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ead05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||å½“ç„¶|å¯ä»¥|ï¼|è¿™é‡Œ|æœ‰ä¸€ä¸ª|å°|ç¬‘è¯|ï¼š\n",
      "\n",
      "|**|ä¸ºä»€ä¹ˆ|ç¨»|è‰|äºº|å¾—äº†|å¥–|ï¼Ÿ|**|  \n",
      "|å› ä¸ºä»–|â€œ|å‡º|ç±»|æ‹”|èƒ|â€|ï¼ˆ|out|standing| in| his| field|ï¼‰|ï¼|  \n",
      "\n",
      "|ğŸ˜„||| å¸Œ|æœ›|è¿™ä¸ª|ç¬‘è¯|èƒ½|è®©ä½ |ä¼š|å¿ƒ|ä¸€ç¬‘|ï¼|å¦‚æœ|éœ€è¦|æ›´å¤š|ï¼Œ|éšæ—¶|å‘Šè¯‰æˆ‘|~||"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm ZZfive, please tell me a joke.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
