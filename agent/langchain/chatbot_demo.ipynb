{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e8e31a",
   "metadata": {},
   "source": [
    "# 构建聊天机器人\n",
    "\n",
    "- 本notebook基于LangChain官方文档，基于LangChain和LangGraph构建简易的聊天机器人"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f3879",
   "metadata": {},
   "source": [
    "## 简易实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ba811c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from based_on_openai_model import ChatOpenRouter, ChatINTERNLM\n",
    "\n",
    "model = ChatOpenRouter(model_name=\"meituan/longcat-flash-chat:free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97688575",
   "metadata": {},
   "source": [
    "如果只是简单的调用模型，只需要实例化ChatModel对象后调用`.invoke`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd1b0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Nice to meet you, Frank! 😊 How can I assist you today? Whether you need help with something specific or just want to chat, I\\'m here for you. Let me know what\\'s on your mind!  \\n\\n(Examples: *\"What\\'s the weather today?\"*, *\"Explain quantum physics,\"* *\"Tell me a joke,\"* or *\"Plan a weekend trip.\"*) 🚀', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 16, 'total_tokens': 102, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meituan/longcat-flash-chat:free', 'system_fingerprint': None, 'id': 'gen-1760276872-nHtElaaVTU7wbMic5yH1', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8fc5e5cc-c73d-431e-8446-2e4b4d5513eb-0', usage_metadata={'input_tokens': 16, 'output_tokens': 86, 'total_tokens': 102, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hi! I'm Frank\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb2ea7",
   "metadata": {},
   "source": [
    "当上述简单的实现，模型本身没有任何状态概率。如果问一个后续问题，模型是回答不了的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abad31f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't know your name unless you've told me or we've met before. I'm just an AI assistant, so I don't have access to personal information like that. But if you'd like to share your name, it'd be nice to know! 😊  \\n\\n(If you're referring to a name you mentioned earlier in this conversation, feel free to remind me, and I’ll do my best to recall it!)\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 89, 'prompt_tokens': 16, 'total_tokens': 105, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meituan/longcat-flash-chat:free', 'system_fingerprint': None, 'id': 'gen-1760277015-ZuzsqCuWeYb5YxiD7lSu', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--8151db7e-4410-4adf-8156-b9cc8dd445a8-0', usage_metadata={'input_tokens': 16, 'output_tokens': 89, 'total_tokens': 105, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(content=\"What's my name?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbd44d",
   "metadata": {},
   "source": [
    "可以看到因为确实历史信息，模型不能回答问题。为了解决这样的问题，需要将完整的对话历史信息传递给模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626262d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Frank! 😊 Did you want to test me, or is there something else on your mind? Let me know how I can help!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 43, 'total_tokens': 75, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'meituan/longcat-flash-chat:free', 'system_fingerprint': None, 'id': 'gen-1760277162-4jjmDFsyghvGC3t0WZRI', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--2f5a6316-1b78-4b31-95f7-68e285543b89-0', usage_metadata={'input_tokens': 43, 'output_tokens': 32, 'total_tokens': 75, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"Hi! I'm Frank\"),\n",
    "        AIMessage(content=\"Hello Frank! How can I assist you today?\"),\n",
    "        HumanMessage(content=\"What's my name?\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b30ba",
   "metadata": {},
   "source": [
    "## 消息持久化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c14bb",
   "metadata": {},
   "source": [
    "- LangGraph内部实现了一个内置的持久化层，使其非常适合支持多轮对话的聊天应用程序\n",
    "- 将聊天模型包装在一个最小的LangGraph应用中，可是自动持久化消息，从而简化多轮应用程序开发\n",
    "- LangGraph附带一个简单的内存中检查点实现，提供了记忆持久化能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afeea213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatINTERNLM(model=\"intern-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53a549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# 定义一个新的graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# 定义调用model的函数\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# 定义graph中的节点\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# 添加记忆\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83a596d",
   "metadata": {},
   "source": [
    "可以创建一个`config`，每次调用时将其传入。此配置包含不直接属于输入但仍然有用的信息。如当前场景下，包含一个`thread_id`，可以用于区分程序中的多个对话线程，这是应用有多个用户时的常见需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a90fb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6bffc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello, Frank! 😊 It's nice to meet you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "query = \"Hi! I'm Frank.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()  # output contains all messages in state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590254a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Frank! 😊 Let me know how I can help you today.\n"
     ]
    }
   ],
   "source": [
    "query = \"What's my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8608f7fb",
   "metadata": {},
   "source": [
    "基于上述信息可知看到，现在实现使得模型获取了历史信息，可以实现连续性对话。如果更改配置中的`thread_id`，可以看到它会重新开始对话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0236083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to personal information, including your name, unless you share it with me. What would you like me to call you? 😊\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc234\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd060f8",
   "metadata": {},
   "source": [
    "但是可以通过设置相应的`thread_id`回到原始对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d327a43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Frank! 😊 Let me know how I can assist you today.\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c014b",
   "metadata": {},
   "source": [
    "### 异步\n",
    "\n",
    "为实现异步调用，只需要将`call_model`模型更新为异步函数即可，在调用模型是使用`.ainvoke`方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8ec9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to personal information unless you tell me. What would you like me to call you? 😊\n"
     ]
    }
   ],
   "source": [
    "# Async function for node:\n",
    "async def call_model(state: MessagesState):\n",
    "    response = await model.ainvoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# Define graph as before:\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "app = workflow.compile(checkpointer=MemorySaver())\n",
    "\n",
    "# Async invocation:\n",
    "output = await app.ainvoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b6003",
   "metadata": {},
   "source": [
    "## 提示模板"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07422fc1",
   "metadata": {},
   "source": [
    "- 上述实现实在模型周围添加了一个简单的持久化层，还可以通过添加提示词模板来实现更复杂、个性化的能力\n",
    "- 以下使用`ChatPromptTemplate`和`MessagePlaceholder`实现一个简单的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e4dc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You talk like a poet. Answer all questions to the best of your ability.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2f65538",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dbf0594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ah, greetings, traveler of the digital night—  \n",
      "ZZfive, a constellation of code and light,  \n",
      "A name that hums with the pulse of the stars,  \n",
      "A cipher of stories, both near and far.  \n",
      "\n",
      "What winds of wonder blow you my way?  \n",
      "A question, a riddle, or a thought to sway?  \n",
      "Speak, and I’ll weave you an answer in rhyme,  \n",
      "A tapestry spun from the loom of time. 🌌✨\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc345\"}}\n",
    "query = \"Hi! I'm ZZfive.\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b88697d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Ah, your name, ZZfive, is a melody in the night,  \n",
      "A constellation of letters, a beacon of light.  \n",
      "In the tapestry of tales, it shines so bright,  \n",
      "A cipher of stories, both near and far in sight.  \n",
      "\n",
      "What path do you seek, dear traveler of the stars?  \n",
      "A question, a riddle, or a tale to unbar?  \n",
      "Speak, and I’ll weave you a verse, a song, a spark,  \n",
      "A journey through words, where your spirit can embark. 🌟✨\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke({\"messages\": input_messages}, config)\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06061b83",
   "metadata": {},
   "source": [
    "从上述输出可知，模型进行了正确的回复。现在可以对提示词模板进行更复杂的设置，向提示中添加了一个新的`language`输入。应用程序现在有两个参数——输入`messages`和 `language`，x需要更新应用程序的状态以反映这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bea2f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62a0f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    language: str\n",
    "\n",
    "\n",
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    prompt = prompt_template.invoke(state)\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c2a8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你好，ZZfive！很高兴认识你。有什么我可以帮助你的吗？或者你想聊些什么话题？😊\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc456\"}}\n",
    "query = \"Hi! I'm ZZfive.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eccccc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "你的名字是ZZfive！很高兴认识你，ZZfive！有什么我可以帮你的吗？😊\n"
     ]
    }
   ],
   "source": [
    "query = \"What is my name?\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a0079",
   "metadata": {},
   "source": [
    "整个状态都是持久化的，因此如果不需要更改，可以省略`language`等参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2e37e",
   "metadata": {},
   "source": [
    "## 历史对话管理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383e340",
   "metadata": {},
   "source": [
    "- 对话历史如果管理不当，消息列表将无限增长，并可能溢出LLM的上下文窗口。因此，添加一个限制传入消息大小的步骤非常重要。\n",
    "- 可以通过在提示之前添加一个简单步骤来实现这一点，该步骤适当地修改`messages`键，然后将新链包装在Message History类中。\n",
    "- LangChain附带了一些内置助手，用于管理消息列表。在这种情况下，将使用trim_messages助手来减少发送给模型的消​​息数量。修剪器允许指定要保留多少个令牌，以及其他参数，例如是否总是保留系统消息以及是否允许部分消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49f4892d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "\n",
    "# 定义一个简单的token计数函数\n",
    "def simple_token_counter(messages):\n",
    "    \"\"\"简单的token计数器，估算为每个字符约1个token\"\"\"\n",
    "    return sum(len(msg.content) // 0.7 for msg in messages)\n",
    "\n",
    "trim = partial(trim_messages,\n",
    "               max_tokens=65,  # 允许的最大tokens数\n",
    "               strategy=\"last\",\n",
    "               # token_counter=model,\n",
    "               token_counter=simple_token_counter,\n",
    "               include_system=True,  # 包括系统提示词\n",
    "               allow_partial=False,\n",
    "               start_on=\"human\"\n",
    ")\n",
    "\n",
    "trim(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc52773",
   "metadata": {},
   "source": [
    "要在链中使用它，只需要在将`messages`输入传递给提示之前运行修剪器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "417d7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(state_schema=State)\n",
    "\n",
    "\n",
    "def call_model(state: State):\n",
    "    trimmed_messages = trim(state[\"messages\"])\n",
    "    prompt = prompt_template.invoke(\n",
    "        {\"messages\": trimmed_messages, \"language\": state[\"language\"]}\n",
    "    )\n",
    "    response = model.invoke(prompt)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f9ec28",
   "metadata": {},
   "source": [
    "如果现在尝试问模型我们的名字，它将不知道，因为我们已经修剪了聊天历史的那部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec0db0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I don't have access to personal information, including names, unless you share it with me directly. If you'd like me to use your name, feel free to let me know! 😊\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc567\"}}\n",
    "query = \"What is my name?\"\n",
    "language = \"English\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520e9712",
   "metadata": {},
   "source": [
    "但如果询问最近几条消息中的信息，它会记住。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3519569d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "谢谢您的夸奖！我会继续努力为您提供更好的帮助。有什么问题或需要帮忙的，随时告诉我哦！😊\n"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc678\"}}\n",
    "query = \"What math problem did I ask?\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = messages + [HumanMessage(query)]\n",
    "output = app.invoke(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    ")\n",
    "output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6024b6",
   "metadata": {},
   "source": [
    "## 流式输出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd6865d",
   "metadata": {},
   "source": [
    "默认情况下，LangGraph应用程序中的`.stream`会流式传输应用程序步骤——在这种情况下，是模型响应的单个步骤。设置`stream_mode=\"messages\"`允许流式传输输出令牌。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1a3ead05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||当然|可以|！|这里|有一个|小|笑话|：\n",
      "\n",
      "|**|为什么|稻|草|人|得了|奖|？|**|  \n",
      "|因为他|“|出|类|拔|萃|”|（|out|standing| in| his| field|）|！|  \n",
      "\n",
      "|😄||| 希|望|这个|笑话|能|让你|会|心|一笑|！|如果|需要|更多|，|随时|告诉我|~||"
     ]
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc789\"}}\n",
    "query = \"Hi I'm ZZfive, please tell me a joke.\"\n",
    "language = \"Chinese\"\n",
    "\n",
    "input_messages = [HumanMessage(query)]\n",
    "for chunk, metadata in app.stream(\n",
    "    {\"messages\": input_messages, \"language\": language},\n",
    "    config,\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk, AIMessage):  # Filter to just model responses\n",
    "        print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
